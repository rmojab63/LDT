---
title: "DC: Credit card fraud detection"
output: rmarkdown::html_vignette
bibliography: ldt.bib
vignette: >
  %\VignetteIndexEntry{DC: Credit card fraud detection}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE} 
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "  >",
  warning = FALSE,
  message = FALSE,
  echo = TRUE,
  results = "hide",
  eval = FALSE
)
```

```{r setup}
library(ldt)
```

It is recommended to read the following vignettes first:

- [SUR: Determinants of long-run economic growth](sur_growth.html)
- [SUR: A simulation](sur_simulation.html)
- [DC: Load default prediction](dc_default.html)

## Introduction
In this vignette, we talk about fraud detection. The data in this example is published as a part of a competition, in which the AUC of the winner model is 0.945884 (see @dataset_ieeefraud). Of course, we are not going to participate in that competition and use a specific test sample and compare the performance of discrete choice modeling to machine learning approaches. This is beyond the scope of this vignette (see, e.g., @clarke2009principles for a discussion). We want to use a large data set, generally to talk about the performance of **ldt** when data is _big_. 

## Data
We use @dataset_ieeefraud and `Data_VestaFraud()` function to get the required data:

```{r data}
vestadata <- Data_VestaFraud(training = TRUE)
```

```{r save_data, echo=FALSE,eval=FALSE}
vig_data = ldt::vig_data
vig_data$vesta = list()
vig_data$vesta$sum_data <- sum(lengths(vestadata$data))
vig_data$vesta$sum_data_na <- sum(is.na(vestadata$data))
vig_data$vesta$ncols <- ncol(vestadata$data)
#usethis::use_data(vig_data, overwrite = TRUE)
```

In this data set, there are two samples: train and test. We will use the training sample in this vignette. The observations are labeled with _fraud_ or _not fraud_. There are 393 features in the files. Furthermore, each observation has an \textit{id} that can link a part of the observations to another data file with 40 identity-related features. The combined data-set has `r ldt::vig_data$vesta$ncols` features and `r round(ldt::vig_data$vesta$sum_data/1000000,0) ` millions data-points in which `r round(ldt::vig_data$vesta$sum_data_na / ldt::vig_data$vesta$sum_data * 100,1) `% is `NA`.

Dependent and potential exogenous data are:
```{r}
y <- as.matrix(vestadata$data[, c("isFraud")])
x <- as.matrix(vestadata$data[, 3:length(vestadata$data)])
weight <- as.matrix((y == 1) * (nrow(y) / sum(y == 1)) + (y == 0))
```

## Estimation

Since the data is large and to increase the speed of the calculations, we change the default optimization options:
```{r}
optimOptions <- GetNewtonOptions(maxIterations = 10, functionTol = 1e-2)
```

We also choose to search a small subset of the model set:

```{r}
xSizes <- list(as.integer(c(1)), as.integer(c(2)), as.integer(c(3)), as.integer(c(4:10)))
xCounts <- c(NA, 20, 15, 10)
```

And, a relatively small out-of-sample simulation:

```{r}
simFixSize <- 4
```

And finally, we search for the best model:

```{r estimation}

vestaRes <- DcSearch_s(
  x = x, y = y, w = weight,
  xSizes = xSizes, counts = xCounts,
  optimOptions = optimOptions,
  searchItems = GetSearchItems(bestK = 20),
  modelCheckItems = GetModelCheckItems(
    maxConditionNumber = 1e15, minDof = 1e5, minOutSim = simFixSize / 2
  ),
  measureOptions = GetMeasureOptions(
    typesIn = c("aucIn"),
    typesOut = c("aucOut"),
    simFixSize = 4,
    trainRatio = 0.9,
    seed = 340
  ),
  searchOptions = GetSearchOptions(printMsg = FALSE),
  printMsg = FALSE,
  savePre = "data/dc_vesta_"
)
```

We can get the in-sample and out-of-sample AUC by the following code:

```{r results='markup'}
print(paste0("Best In-Sample AUC:     ", vestaRes$aucIn$target1$model$bests$best1$weight))
print(paste0("Best Out-Of-Sample AUC: ", vestaRes$aucOut$target1$model$bests$best1$weight))
```

Note that we do not evaluate the codes here due to the large data.

## Discussion
The computations are relatively time-consuming even if we choose a small subset of the model set. One might be able to improve our current performance by better use of categorical features (e.g., by studying them and grouping some items). Furthermore, (for development) any improvement in the speed of the calculations allows us to search a larger proportion of the data set. In the presence of categorical variables, applying a more efficient algorithm in dealing with dummy variables or sparse matrices might be helpful.


## References


