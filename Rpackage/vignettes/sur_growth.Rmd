---
title: "SUR: Determinants of long-run economic growth"
output: rmarkdown::html_vignette
bibliography: ldt.bib
vignette: >
  %\VignetteIndexEntry{SUR: Determinants of long-run economic growth}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "  >",
  warning = FALSE,
  message = FALSE,
  echo = TRUE,
  results = 'hide',
  eval = TRUE 
)
```

```{r setup}
library(ldt)
library(kableExtra)
```

## Introduction

This vignette is an example in which we talk about the determinants of GDP per capita in different economies in the long run. In a typical application in **ldt**, the goal is to _automatically explain_ or _automatically predict_ one or more than one random variable. This example is about the former. 

We try to explain the _long-run yearly growth rates_ of GDP per capita by using the long-run yearly growth rates of other variables. These _potential explanatory variables_ (or _potential regressors_) are generally related to macroeconomics. More information is provided in the other sections. 

<p style="margin-left:40px;">
What is the long-run yearly growth rate of a variable? Let $x$ be a level variable for which a time-series sample $\{x_t\}_{0}^{s}$ with yearly frequency is available. We define (continuous) long-run yearly growth rate to be given by $\log(x_s/x_0)/s\times 100$ formula.
</p>

One complication that arises is the endogeneity problem. To avoid this, one approach is to split the data and try to explain the growth after a specific point in time by using the information before that point. Due to the properties of the data (which is explained in the next section), we choose the following split year:

```{r split_year}
splitYear <- 2005
```


This means that the goal is to explain the long-run GDP per capita growth after `r splitYear` by using the information before this year. By information before `r splitYear`, we mean the long-run growth rate of all available data, including the GDP per capita itself. 



## Data

One of the main ideas behind **ldt** is to minimize the user's discretion. Therefore, like many other applications in **ldt**, this example is based on a _data-set_ and a set of rules that converts this data set to a list of _potential regressors and/or predictors_. Note that, apart from avoiding discretion, a rule-based approach in selecting the data might be expected because of the word: _automatically_ (see the previous section).  

Anyway, we select the @dataset_worldbank0 and automatically create the table of the potential regressors. For this particular data set, we use `ldt::Data_Wdi()` function. Note that for replication, you should download the data set from the WDI website. See the documentation of `Data_Wdi()` function for more information. Note that the size of the WDI data set is large and `Data_Wdi()` function is generally time-consuming.

```{r create_data_1, eval=FALSE}
wdi_lr_2005 <- Data_Wdi(
  dirPath = path_to_wdi_directory,
  minYear = 1960, maxYear = splitYear,
  aggFunction = function(data, code, name, unit, definition, aggMethod) {
    isPerc <- unit == "%" || grepl(".ZG", code)
    if (isPerc) NA else LongrunGrowth(data, 20, 2, FALSE, TRUE, isPerc)
  },
  keepFunction = function(X) {
    var(X, na.rm = TRUE) > 1e-12 && sum((is.na(X)) == FALSE) >= 50
  }
)

```

These lines of code aggregate the data between 1960 and `r splitYear`. The `aggFunction` argument defines the aggregation type. It uses different types of information (e.g., `unit` or `code`) and excludes some series from the analysis. In this implementation, if `unit` of a series is a percentage, it is excluded from the analysis. Note that, in order to calculate the long-run growth rates, we use `ldt::LongrunGrowth()` function. Since we are dealing with an unclean data set, this function helps us to handle missing data-points (see the documentation). Furthermore, `keepFunction` argument excludes series with relatively large number of `NA`s or relatively small variances.

As it is pointed out in the previous section, we need two data-tables, one with data before `r splitYear` and the other with data after this year. Dependent variable is the long-run GDP per capita growth after `r splitYear`. We use a similar code to create it: 

```{r get_create_data_2, eval=FALSE}
wdi_lr_2006_21 <- Data_Wdi(
  dirPath = path_to_wdi_directory,
  minYear = splitYear + 1, maxYear = 2021,
  aggFunction = function(data, code, name, unit, definition, aggMethod) {
    isPerc <- unit == "%" || grepl(".ZG", code)
    if (isPerc) NA else LongrunGrowth(data, 2, 4, FALSE, TRUE, isPerc)
  },
  keepFunction = function(X) {
    var(X, na.rm = TRUE) > 1e-12 && sum((is.na(X)) == FALSE) >= 50
  }
) 

```

What remains is cleaning and rearranging data. we use `ldt::Data_WdiSearchFor()` function to find the target variable:

```{r data_find_target, eval=FALSE}
serCode <- Data_WdiSearchFor(wdi_lr_2005$series, c("GDP", "per capita", "constant", "us"))[[1]]$code
y <- wdi_lr_2006_21$data[, which(colnames(wdi_lr_2006_21$data) %in% c(serCode)), drop = FALSE]
x <- as.data.frame(wdi_lr_2005$data)
```

We remove highly correlated columns and non-country observations:

```{r data_remove_noncountry, eval=FALSE}
x <- x[, which(sapply(x, function(v) abs(cor(y, v, use = "complete.obs")) < 0.999)), drop = FALSE]
y <- y[wdi_lr_2005$countries$isCountry, , drop = FALSE]
x <- x[wdi_lr_2005$countries$isCountry, , drop = FALSE]
if (any(rownames(x) != names(y))) {
  stop("Invalid operation.")
}
```

We rearrange data such that the lag of the dependent variable is in the first column. Finally, we add intercept to be one of the potential regressors:

```{r data_rearrange, eval=FALSE}
ind <- which(colnames(x) %in% c(serCode))
x <- cbind(x[, ind, drop = F], x[, 1:(ind - 1), drop = F], x[, ((ind + 1):ncol(x)), drop = F])
x <- cbind(matrix(rep(1, nrow(x)), nrow(x), dimnames = list(NULL, "Intercept")), x)
```

A this point, `x` is a table with countries in the rows and variables in the columns. 

## Model-set
In this example, there are one target variable and (originally) `r ldt::vig_data$wdi$nxcol_orig` potential regressors. Different combinations of the variables create different (potential) regression models. The set of all linear regression models is relatively large. For example, the following code approximates the size of such a set in which the regressions have at least 6 explanatory variables:

```{r modelset_size, results='markup'}
n0 = vig_data$wdi$nxcol_orig
n0Choose6 <- sum(sapply(seq(1:6), function(i) choose(n0, i)))
print(prettyNum(n0Choose6, big.mark = ","))
```

In other words, our generosity in defining new potential regressors is computationally (and exponentially) costly. We can restrict ourselves to small regression models or use theoretical information to exclude a part of the potential regressors from the analysis.

### Which subset of the model set should we estimate?

Since **ldt** tries to be an atheoretical tool, we restrict the model set. `ldt::SurSearch_s()` function has a step-wise search algorithm. It estimates small regressions, and given a list of criteria, it omits regressors with poor performance. In other words, larger models are not built with variables with relatively low explanatory power in the smaller models.

The steps of `SurSearch\_s()` function is defined by a list of indices and a vector of sizes: 

```{r modelset_steps}
xSizes <- list(c(2L,3L), c(4L))
xCounts <- c(NA, 5)
```

This means that at the first step, we use all data and estimate models with 2 potential regressors; Next, we select `r xCounts[[2]]` _best_ potential regressors and estimate models with 4 potential regressors.

### What do we mean by _best_ potential regressors? 

For a given regression model, **ldt** calculates two types of scores: in-sample and out-of-sample. The latter is calculated in a pseudo-out-of-sample simulation (e.g., by calculating the distance between the projected value and the actual one), and the former is calculated by using all the sample data (e.g., AIC of the regression). We select a set of options by the following code:

```{r modelset_measure_options}
measureOptions <- GetMeasureOptions(
  typesIn = c("aic", "sic"), typesOut = c("rmse", "crps"),
  simFixSize = 60, trainRatio = 0.75, seed = 340
)
```

In other words, we are asking **ldt** to use AIC and SIC as the two in-sample criteria, and RMSE and CRPS as the two out-of-sample ones. There are `r measureOptions$simFixSize` number of iterations in the simulation and `r measureOptions$seed` is used as the seed. In each iteration, `r measureOptions$trainRatio*100` percent of the sample is used for estimation and the rest is used for testing.

### What type of information are we looking for?
Given a regression model, one might discuss the value of the estimated coefficients, its sign or statistical significance, or one might use it for projection. When model uncertainty presents, one might also look for inclusion weights. While **ldt** provides a range of options, in this example, we focus on the following:

```{r modelset_search_items}
searchItems <- GetSearchItems(model = TRUE, bestK = 50, inclusion = TRUE)
```

In other words, we are asking **ldt** to keep the first `r searchItems$bestK` best models and also, the inclusion weights.

### How can we deal with high levels of multicollinearity?
When it comes to an automatic approach toward modeling, We might want to exclude regressions with high levels of multicollinearity, low degrees of freedom (in presence of `NA`s), relatively large AIC, etc. **ldt** provides some options in this regard. In the current example, we use the following code:

```{r modelset_check_items}
checkItems <- GetModelCheckItems(maxConditionNumber = 1e15, minDof = 35, minOutSim = 50)
```

In other words, we are asking **ldt** to omit a regression with a degrees-of-freedom less than `r checkItems$minDof`, a condition number larger than `r checkItems$maxConditionNumber`, or a number of valid out-of-sample simulations less than `r checkItems$minOutSim`. We hope that restricting the condition number can deal with the multicollinearity problem. 

Another approach in dealing with multicollinearity is to partition the regressors and skip the regressions with at least two regressors in the same partition. We can use theory to create the partitions or we can cluster the variables based on a specific statistical distance. Note that such partitioning reduces the number of potential models and therefore speeds up the calculations. Of course, in this example partitions have just one variable.

### How can we estimate all models with an intercept?
We would like to include the intercept and the lag of the dependent variable in all the regressions. **ldt** allows us to fix the first $k$ partitions. Since the two first variables are intercept and the lag of the dependent variable, in the next section, we set `numFixXPartitions` argument to be 2.

## Estimation

We are not able to load the data in this vignette, because it needs an external data set and this is not available in this package. But, a part of the data set is saved in the **ldt** package, and we load:
```{r data_load}
x = as.matrix(ldt::vig_data$wdi$x)
y = as.matrix(ldt::vig_data$wdi$y)
```
If you have downloaded the data set files, do not run this code.


```{r estimation}
search_res <- SurSearch_s(
  x = x, y = y, numFixXPartitions = 2,
  xSizes = xSizes, counts = xCounts,
  searchItems = searchItems, modelCheckItems = checkItems, measureOptions = measureOptions,
  searchOptions = GetSearchOptions(parallel = TRUE, printMsg = FALSE)
)
```

The arguments are discussed in the previous sections. Note that `SurSearch_s()` can save the result of different steps in the working directory. 

### Status

We can print the results to get some general information about the search process. It prints the number of valid or failed estimations or the weight of the best models:

```{r estimation_print, results='hold'}
print(search_res)
```

Note that the weights are related to the in-sample and out-of-sample criteria discussed before. See the documentation of `ldt::GetWeightFromMeasure()` or `GetMeasureFromWeight()` functions. Also, an error occurred in `r round(search_res$counts$failedCount/search_res$counts$searchedCount*100,1)`% of the searched models. Note that this number may increase if we apply more strict options in `modelCheckItems` argument.

### Best models
While the best weights are informative, we generally want more information about the best models. The returned value (which is an `ldtsearch` object) provides some general information such as the indices of the exogenous variables. The `summary` function can be used to estimate the best models:

```{r estimation_summary, results='markup'}
search_res_sum <- summary(search_res, y = y, x = x, test = FALSE, printMsg = FALSE)
```

However, this returned value contains lots of information and we need a way of summarizing them. If the goal is to discuss the coefficients, `ldt::CoefTable()` function can be used to create a table. First, we need to create a list of models:

```{r estimation_table_1}
best_models <- list(
  aic = search_res_sum$aic$target1$model$bests$best1,
  rmse = search_res_sum$rmse$target1$model$bests$best1,
  crps = search_res_sum$crps$target1$model$bests$best1
)
```

Next, we should choose the types of information from the regressions, we like to display. It gets done based on some keywords from the documentation of the `CoefTable()` function. For example:

```{r estimation_table_2}
regInfo <- list(
  c("", ""), c("num_obs", "No. Obs."), c("num_x", "No. Exo."), c("sigma2", "S.E. of Reg."), 
  c("r2", "R Sq."), c("aic", "AIC"), c("sic", "SIC"), c("rmse", "RMSE"), c("crps", "CRPS")
)
```

Next, we retrieve the names of the variables from the data set. We also need some adjustments to escape some characters and to get a more informative table: 

``` {r estimation_table_3}
vnamefun_wdi <- function(x) {
  tryCatch(
    {
      Data_WdiSearchFor(wdi_lr_2005$series, keywords = c(x), searchName = FALSE, findOne = TRUE)$name
    },
    error = function(e) x
  )
}
hnameFun <- function(x) {
  paste0((if (x == "aic") "AIC" else if (x == "sic") "SIC" else if (x == "rmse") "RMSE" else if (x == "crps") "CRPS"), "-based")
}
replaces <- list(c("US$", "US\\$"))
```

Finally, we can get the table and draw it by using **kableExtra** package:

```{r estimation_table_4}
res <- CoefTable(best_models,
                 tableFun = "coef_star", hnameFun = hnameFun, vnamesFun = vnamefun_wdi, regInfo = regInfo,
                 vnamesFun_sub = replaces, vnamesFun_max = 40, formatNumFun = function(j,x)round(x,3),
                 formatLatex = FALSE
)
```

```{r estimation_table_kable, echo=FALSE, results='markup'}
kableExtra::kable(res, "html", digits = 2,
                  booktabs = TRUE, label = "long-run-growth",
                  align = c("c", "c", "c"), caption = "Long-run GDP per capita growth: best regressions",
                  linesep = "", escape = FALSE
) %>%
  kableExtra::footnote(
    general = "<p>Dependent variable is the long-run yearly growth rates of GDP per capita of different countries after 2006. Regressors are selected automatically. Column headers indicate the criteria used to select the best regression. Data is from WDI. Parameters of the equations are reported at the top and other properties at the bottom. `***`, `**`, and `*` denotes significance level of 1%, 5% and 10%, respectively.</p>", escape = FALSE, fixed_small_size = TRUE, footnote_as_chunk = TRUE
  ) %>%
  kable_styling(full_width = NULL)
```

Note that the goal of this vignette is not to interpret or discuss the coefficients of the best models.

## Principle component analysis
Another practical approach in dealing with a large number of potential regressors in a regression analysis is to use Principal Component Analysis (PCA). The estimation process is much faster than the previous search approach. However, there are some drawbacks. For example, the results might be less instructive, and missing observations or determining the number of principal components might become a problem. 

To calculate the principle components, we should omit incomplete observations (rows with at least one `NA`). However, before removing the rows, we can omit a variable with a relatively large number of missing data points (columns with many `NA`s). In other words, to prepare the data for PCA, there is a trade-off between the number of variables and the number of observations. One approach is to try to maximize the overall number of observations. In this example, we use `ldt::RemoveNaStrategies()` and estimate two types of models: one with a relatively large number of observations, and another one with a relatively large number of explanatory variables. First, we get the available strategies:

```{r pca_na_strategies}
x_ <- x[, 3:ncol(x), drop = FALSE]
strategies <- RemoveNaStrategies(x_)
```

Each strategy specifies the number of rows or columns that should be removed and also the order. Note that we want to exclude the intercept and the lag of the dependent variable from PCA. The `NA` strategies are sorted by the overall number of observations. We can print the number of variables and select some of them:

```{r pca_na_strategies_print, results='markup'}
cat(unlist(sapply(strategies, function(s) s$nCols))[1:20], " ...\n")
```

These numbers are the number of variables in each strategy. While the choice is not straightforward, we choose the first strategy (with `r strategies[[1]]$nCols` variables and `r strategies[[1]]$nCols*strategies[[1]]$nRows` observation) and the 2nd strategy (with `r strategies[[2]]$nCols` variables and `r strategies[[2]]$nCols * strategies[[2]]$nRows` observation) to fulfill our goal. Of course, since we want a sensitivity analysis on the number of principal components too, we select two values as the thresholds of the cumulative variance ratio. In other words, we use the following combination:

```{r pca_na_strategies_select}
inds <- c(1, 2, 1, 2)
pcaCutoffRates <- c(0.95, 0.95, 0.6, 0.6)
xPcaOptions <- GetPcaOptions(ignoreFirst = 2, exactCount = 0, max = 8)
```

This means we will estimate 6 models by combining the first two `NA` strategies and three cumulative variance ratios: 0.95, 0.9, and 0.6. The third line of code is just a setup for ignoring the first two regressors and choosing the number of PCs while estimating the model. We restrict the number of PCs to be less than `r xPcaOptions$max`. This is a relatively large number and therefore, as the next step, we ask **ldt** to omit insignificant coefficients and re-estimate the model, and repeat this process 2 times:

```{r pca_significance_options}
searchSigMaxIter <- 2
searchSigMaxProb <- 0.2
```

Note that we choose a relatively large value for the type-I error (i.e., `r searchSigMaxProb`). Next, we prepare the data for each case and use `ldt::SurEstim()` function to estimate and create a list of the estimated models:

```{r pca_estimation}
pca_models <- list()
i <- 0
for (ind in inds) {
  i <- i + 1
  strat <- strategies[[ind]]
  x0 <- NULL
  y0 <- y[-strat$rowRemove, , drop = FALSE]
  if (strat$colFirst) {
    if (length(strat$colRemove)>0)
      x0 <- x_[, -strat$colRemove, drop = FALSE]
    x0 <- x0[-strat$rowRemove, , drop = FALSE]
  } else {
    x0 <- x_[-strat$rowRemove, , drop = FALSE]
    if (length(strat$colRemove)>0)
      x0 <- x0[, -strat$colRemove, drop = FALSE]
  }
  x0 <- cbind(x[, c(1, 2), drop = F][-strat$rowRemove, , drop = FALSE], x0)
  xPcaOptions$cutoffRate <- pcaCutoffRates[[i]]
  pca_models[[length(pca_models) + 1]] <- SurEstim(y0, x0,
                                                   addIntercept = FALSE,
                                                   searchSigMaxIter = searchSigMaxIter,
                                                   searchSigMaxProb = searchSigMaxProb,
                                                   pcaOptionsX = xPcaOptions,
                                                   simFixSize = measureOptions$simFixSize,
                                                   simTrainRatio = measureOptions$trainRatio, simSeed = abs(measureOptions$seed),
                                                   simMaxConditionNumber = checkItems$maxConditionNumber,
                                                   printMsg = FALSE
  )
}
```

Note that the main difficulty here is to prepare the data. It arises because we want to exclude the first two variables from the PCA. Also, note that some options such as the number of simulations are defined in the previous section. 

Finally, similar to the previous section, we use the list of estimations and create a table:

```{r pca_prepare_table}
regInfo <- append(regInfo, 
                  list(c("num_exo", "No. Exo. (orig.)"), 
                       c("pca_x_cutoff", "PCA Cutoff")), 2)
regInfo <- append(regInfo, list(c("", "[. . .]")), 0)
regInfo <- append(regInfo, list(c("num_rest", "No. Rest.")), 6)
res <- CoefTable(pca_models,
                 tableFun = "coef_star", hnameFun = function(x) x, vnamesFun_sub = replaces,
                 vnamesFun = vnamefun_wdi, vnamesFun_max = 16, regInfo = regInfo, 
                 numCoefs = 2, formatNumFun = function(j,x)round(x,3),
                 formatLatex = FALSE
)
colnames(res) <- paste0("mod.", seq(1, length(pca_models))) 
```

```{r pca_table_kable, echo=FALSE, results='markup'}
kableExtra::kable(res, "html",
                  booktabs = TRUE, label = "long-run-growth-pca",
                  align = c(rep("c", 6)), caption = "Long-run GDP per capita growth: regressions with PCA",
                  linesep = "", escape = FALSE
) %>%
  kableExtra::footnote(
    general = "<p>Dependent variable is the long-run yearly growth rates of GDP per capita of different countries after 2006. Regressors are the PC of different combinations of the variables. Each column belongs to a specific strategy for removing the `NA`s and selecting the number of PCs. The intercept and the lag of the dependent variable are excluded from PCA. Other coefficients are not reported. Data is from WDI. `***`, `**`, and `*` denotes significance level of 1%, 5% and 10%, respectively. `(r)` means the coefficient is restricted.</p>", escape = FALSE, fixed_small_size = TRUE, footnote_as_chunk = TRUE
  ) %>%
  kable_styling(full_width = NULL)
```

Again the goal of this vignette is a presentation and therefore, I do not compare or discuss the results.


## References
